{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b146f003",
   "metadata": {
    "id": "81f38f35",
    "papermill": {
     "duration": 0.003248,
     "end_time": "2025-06-27T14:02:27.015303",
     "exception": false,
     "start_time": "2025-06-27T14:02:27.012055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e786fc",
   "metadata": {
    "id": "UA640DrrQ1U-",
    "outputId": "1ac3c1d0-d33b-4a46-8589-304bd0e3ee9e",
    "papermill": {
     "duration": 91.231663,
     "end_time": "2025-06-27T14:03:58.249779",
     "exception": false,
     "start_time": "2025-06-27T14:02:27.018116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c5ab9",
   "metadata": {
    "id": "TB3DfNZzQwH_",
    "outputId": "63326c41-460e-44b3-dc7d-282cc105c629",
    "papermill": {
     "duration": 8.801367,
     "end_time": "2025-06-27T14:04:07.077858",
     "exception": false,
     "start_time": "2025-06-27T14:03:58.276491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import shutil\n",
    "import random\n",
    "import yaml\n",
    "import clip\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5a616",
   "metadata": {
    "id": "Ubq5P3UzZzQE",
    "papermill": {
     "duration": 0.040417,
     "end_time": "2025-06-27T14:04:07.142958",
     "exception": false,
     "start_time": "2025-06-27T14:04:07.102541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union, List, Tuple\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "\n",
    "class ClipEmbedder:\n",
    "    \"\"\"\n",
    "    A high-performance embedding generator using OpenAI's CLIP models.\n",
    "    Efficiently converts images and texts into semantic vector representations.\n",
    "\n",
    "    Attributes:\n",
    "        model (torch.nn.Module): Pre-trained CLIP model\n",
    "        preprocess (callable): Image preprocessing pipeline\n",
    "        device (torch.device): Computation device\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clip_model_name: str = \"ViT-B/32\", device: str = \"\"):\n",
    "        \"\"\"\n",
    "        Initialize CLIP model and preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            clip_model_name (str): CLIP model variant (default: \"ViT-B/32\")\n",
    "            device (str): Hardware device for computation\n",
    "        \"\"\"\n",
    "        # Load CLIP model and preprocessing\n",
    "        self.model, self.preprocess = clip.load(clip_model_name)\n",
    "\n",
    "        # Configure device\n",
    "        self.device = device.lower()\n",
    "        if not self.device:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Move model to device\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_image_embeddings(\n",
    "        self,\n",
    "        images: Union[Image.Image, np.ndarray, List[Union[Image.Image, np.ndarray]]],\n",
    "        batch_size: int = 32\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for input images with efficient batch processing.\n",
    "\n",
    "        Args:\n",
    "            images: Single PIL.Image or list of PIL.Images\n",
    "            batch_size: Processing batch size (optimize based on GPU memory)\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Embedding matrix of shape (n_images, embedding_dim)\n",
    "\n",
    "        Note: Embeddings are L2-normalized per CLIP's standard practice\n",
    "        \"\"\"\n",
    "        # Normalize input to list\n",
    "        if isinstance(images, Image.Image):\n",
    "            images = [images]\n",
    "        if isinstance(images, np.ndarray):\n",
    "            images = [images]\n",
    "\n",
    "        # Handle empty input\n",
    "        if len(images) == 0:\n",
    "            return np.array([])\n",
    "\n",
    "        image_embeddings = []\n",
    "\n",
    "        # Batch processing loop\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i + batch_size]\n",
    "            batch_tensors = []\n",
    "\n",
    "            for img in batch:\n",
    "                # Convert cv2 image (BGR) to PIL Image (RGB)\n",
    "                if isinstance(img, np.ndarray):\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = Image.fromarray(img)\n",
    "\n",
    "                # Preprocess image\n",
    "                img_tensor = self.preprocess(img)\n",
    "                batch_tensors.append(img_tensor)\n",
    "            # Stack tensors and move to device\n",
    "            batch_tensors = torch.stack(batch_tensors).to(self.device)\n",
    "            # Generate embeddings\n",
    "            with torch.no_grad():\n",
    "                batch_embeds = self.model.encode_image(batch_tensors)\n",
    "                batch_embeds /= batch_embeds.norm(dim=-1, keepdim=True)  # L2 normalization\n",
    "                image_embeddings.append(batch_embeds.cpu().numpy())\n",
    "        # Combine all batch results\n",
    "        return np.vstack(image_embeddings)\n",
    "\n",
    "    def get_texts_embeddings(\n",
    "            self,\n",
    "            texts: Union[str, List[str]],\n",
    "            batch_size: int = 128,\n",
    "            context_length: int = 77\n",
    "        ) -> np.ndarray:\n",
    "            \"\"\"\n",
    "            Generate embeddings for input text with efficient batch processing.\n",
    "\n",
    "            Args:\n",
    "                texts: Input string or list of strings\n",
    "                batch_size: Text processing batch size (typically larger than image batches)\n",
    "                context_length: Override default token limit (77 tokens)\n",
    "\n",
    "            Returns:\n",
    "                L2-normalized embeddings as numpy array (n_texts, embedding_dim)\n",
    "            \"\"\"\n",
    "            # Input normalization\n",
    "            texts = [texts] if isinstance(texts, str) else texts\n",
    "            if not texts:\n",
    "                return np.array([])\n",
    "\n",
    "            # Use model's context length if not specified\n",
    "            context_length = context_length\n",
    "\n",
    "            embeddings = []\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i + batch_size]\n",
    "\n",
    "                # Tokenize with truncation\n",
    "                batch_tokens = clip.tokenize(\n",
    "                    batch,\n",
    "                    truncate=True,\n",
    "                    context_length=context_length\n",
    "                ).to(self.device)\n",
    "\n",
    "                # Generate embeddings\n",
    "                with torch.no_grad():\n",
    "                    batch_embeds = self.model.encode_text(batch_tokens)\n",
    "                    batch_embeds /= batch_embeds.norm(dim=-1, keepdim=True)\n",
    "                    embeddings.append(batch_embeds.cpu().numpy())\n",
    "\n",
    "            return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2e579",
   "metadata": {
    "id": "HL581jEVZ1Y8",
    "papermill": {
     "duration": 0.073206,
     "end_time": "2025-06-27T14:04:07.256451",
     "exception": false,
     "start_time": "2025-06-27T14:04:07.183245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from ultralytics import YOLO\n",
    "from ultralytics import SAM\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "class FashionSegmentationModel:\n",
    "    \"\"\"\n",
    "    A comprehensive model for detecting and segmenting fashion items in images.\n",
    "    Combines YOLOv8 for object detection and Segment Anything Model (SAM) for\n",
    "    high-precision segmentation. Provides visualization utilities and standardized\n",
    "    output generation.\n",
    "\n",
    "    Attributes:\n",
    "        detection_model (YOLO): Pretrained YOLOv8 detection model\n",
    "        segmentation_model (SAM): Pretrained SAM segmentation model\n",
    "        device (str): Computation device (e.g., 'cpu', 'cuda')\n",
    "\n",
    "    Typical usage:\n",
    "        model = FashionSegmentationModel(\"yolo.pt\", \"sam.pt\", \"cuda\")\n",
    "        segments = model.get_segment_images(\"fashion.jpg\")\n",
    "        model.visualize_segments(\"fashion.jpg\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, yolo_model_path: str, sam_model_path: str, device: str = \"\"):\n",
    "        \"\"\"\n",
    "        Initialize detection and segmentation models.\n",
    "\n",
    "        Args:\n",
    "            yolo_model_path: Path to YOLOv8 .pt weights file\n",
    "            sam_model_path: Path to SAM .pt weights file\n",
    "            device: Hardware device for inference ('' for auto-detection)\n",
    "        \"\"\"\n",
    "        self.detection_model = YOLO(yolo_model_path)\n",
    "        self.segmentation_model = SAM(sam_model_path)\n",
    "        self.device = device\n",
    "\n",
    "    def _detect_clothes(self, img_path: str) -> List[Tuple[str, List[int]]]:\n",
    "        \"\"\"\n",
    "        Detect fashion items and return bounding box coordinates.\n",
    "\n",
    "        Args:\n",
    "            img_path: Path to input image\n",
    "\n",
    "        Returns:\n",
    "            List of tuples (class_name, [xmin, ymin, xmax, ymax])\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If duplicate clothing classes are detected\n",
    "\n",
    "        Process:\n",
    "            1. Load image and get dimensions\n",
    "            2. Run YOLO detection\n",
    "            3. Convert center-based coordinates to corner coordinates\n",
    "            4. Validate unique class detection\n",
    "        \"\"\"\n",
    "        image = cv2.imread(img_path)\n",
    "        img_height, img_width = image.shape[:2]\n",
    "\n",
    "        # Clothing class mapping\n",
    "        classes = {\n",
    "            0: 'sunglass', 1: 'hat', 2: 'jacket', 3: 'shirt',\n",
    "            4: 'pants', 5: 'shorts', 6: 'skirt', 7: 'dress',\n",
    "            8: 'bag', 9: 'shoe'\n",
    "        }\n",
    "\n",
    "        # Perform detection\n",
    "        clothes = self.detection_model.predict(img_path)\n",
    "        bounding_boxes = clothes[0].boxes.cpu().numpy()\n",
    "\n",
    "        # Process detections\n",
    "        cloth_labels = []\n",
    "        for cloth_class, cloth_box in zip(bounding_boxes.cls, bounding_boxes.xywh):\n",
    "            cloth_labels.append([cloth_class] + list(cloth_box))\n",
    "\n",
    "        detected_clothes = []\n",
    "        found_clothes = set()\n",
    "\n",
    "        for label in cloth_labels:\n",
    "            name, x, y, width, height = label\n",
    "            name = int(name)\n",
    "\n",
    "            # Skip 'bag' class due to obvious and numerous artefacts\n",
    "            if name == 8:\n",
    "                continue\n",
    "\n",
    "            # Validate unique classes\n",
    "            if name in found_clothes:\n",
    "                raise ValueError(f\"Duplicate {classes[name]} detected!\")\n",
    "            found_clothes.add(name)\n",
    "\n",
    "            # Convert to corner coordinates\n",
    "            name = classes[name]\n",
    "            xmin = max(0, int(x - width/2))\n",
    "            xmax = min(img_width, int(x + width/2))\n",
    "            ymin = max(0, int(y - height/2))\n",
    "            ymax = min(img_height, int(y + height/2))\n",
    "\n",
    "            detected_clothes.append((name, [xmin, ymin, xmax, ymax]))\n",
    "\n",
    "        return detected_clothes\n",
    "\n",
    "    def segment_clothes(self, img_path: str) -> Tuple[List[np.ndarray], List[str]]:\n",
    "        \"\"\"\n",
    "        Perform segmentation on detected fashion items.\n",
    "\n",
    "        Args:\n",
    "            img_path: Path to input image\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - List of normalized segmentation polygons (xyn format)\n",
    "                - List of clothing class names\n",
    "        \"\"\"\n",
    "        detected_clothes = self._detect_clothes(img_path)\n",
    "        if len(detected_clothes) == 0:\n",
    "          return ([], [])\n",
    "        bounding_boxes = [item[1] for item in detected_clothes]\n",
    "        cloth_names = [item[0] for item in detected_clothes]\n",
    "\n",
    "        # Run segmentation\n",
    "        segmentation_result = self.segmentation_model.predict(\n",
    "            img_path,\n",
    "            bboxes=bounding_boxes,\n",
    "            verbose=False,\n",
    "            save=False,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # Extract normalized polygons\n",
    "        segments = segmentation_result[0].masks.xyn\n",
    "        return segments, cloth_names\n",
    "\n",
    "    def visualize_bounding_boxes(self, img_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Visualize detected bounding boxes with cropped regions.\n",
    "\n",
    "        Args:\n",
    "            img_path: Path to input image\n",
    "\n",
    "        Output:\n",
    "            Grid plot (max 4 items) showing cropped regions with class labels\n",
    "        \"\"\"\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        detected_clothes = self._detect_clothes(img_path)\n",
    "        if len(detected_clothes) == 0:\n",
    "          return None\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, cloth in enumerate(detected_clothes):\n",
    "            name, (xmin, ymin, xmax, ymax) = cloth\n",
    "            cropped = image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            plt.imshow(cropped)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'{name}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_segments(self, img_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Comprehensive visualization of segmentation results.\n",
    "\n",
    "        Three-panel visualization:\n",
    "        1. Original image with bounding boxes and class labels\n",
    "        2. Segmentation contours overlaid on original image\n",
    "        3. Combined segmentation mask with color coding\n",
    "\n",
    "        Args:\n",
    "            img_path: Path to input image\n",
    "        \"\"\"\n",
    "        segments, cloth_names = self.segment_clothes(img_path)\n",
    "        if len(segments) == 0:\n",
    "          return None\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        original_h, original_w = image.shape[:2]\n",
    "\n",
    "        # Create copy for contour drawing\n",
    "        contour_image = image.copy()\n",
    "\n",
    "        # Initialize figure\n",
    "        plt.figure(figsize=(18, 10))\n",
    "\n",
    "        # Panel 1: Bounding boxes\n",
    "        plt.subplot(131)\n",
    "        display_img = image.copy()\n",
    "        detected_bboxes = [item[1] for item in self._detect_clothes(img_path)]\n",
    "\n",
    "        for i, (name, bbox) in enumerate(zip(cloth_names, detected_bboxes)):\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            cv2.rectangle(display_img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            cv2.putText(display_img, name, (xmin, ymin-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        plt.imshow(display_img)\n",
    "        plt.title('Detected Objects')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Panel 2: Segmentation contours\n",
    "        plt.subplot(132)\n",
    "        for i, segment in enumerate(segments):\n",
    "            # Validate segment data\n",
    "            if segment is None or len(segment) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Convert normalized to absolute coordinates\n",
    "            absolute_segment = segment.copy()\n",
    "            \n",
    "            # Check for invalid coordinates\n",
    "            if np.any(np.isnan(absolute_segment)) or np.any(np.isinf(absolute_segment)):\n",
    "                continue\n",
    "                \n",
    "            absolute_segment[:, 0] *= original_w\n",
    "            absolute_segment[:, 1] *= original_h\n",
    "\n",
    "            points = absolute_segment.reshape((-1, 1, 2)).astype(np.int32)\n",
    "            \n",
    "            # Validate points\n",
    "            if len(points) < 3 or points.shape[1] != 1 or points.shape[2] != 2:\n",
    "                continue\n",
    "                \n",
    "            # Clamp points to image boundaries\n",
    "            points[:, 0, 0] = np.clip(points[:, 0, 0], 0, original_w-1)\n",
    "            points[:, 0, 1] = np.clip(points[:, 0, 1], 0, original_h-1)\n",
    "\n",
    "            try:\n",
    "                # Draw contours\n",
    "                cv2.polylines(contour_image, [points], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "                # Add text labels at centroid\n",
    "                M = cv2.moments(points)\n",
    "                if M[\"m00\"] != 0:\n",
    "                    cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                    cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "                    cv2.putText(contour_image, cloth_names[i], (cX, cY),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            except cv2.error as e:\n",
    "                print(f\"OpenCV contour drawing error: {e}. Skipping this segment.\")\n",
    "                continue\n",
    "\n",
    "        plt.imshow(contour_image)\n",
    "        plt.title('Segmentation Contours')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Panel 3: Combined mask\n",
    "        plt.subplot(133)\n",
    "        combined_mask = np.zeros((original_h, original_w), dtype=np.uint8)\n",
    "\n",
    "        for i, segment in enumerate(segments):\n",
    "            # Validate segment data\n",
    "            if segment is None or len(segment) == 0:\n",
    "                continue\n",
    "                \n",
    "            absolute_segment = segment.copy()\n",
    "            \n",
    "            # Check for invalid coordinates\n",
    "            if np.any(np.isnan(absolute_segment)) or np.any(np.isinf(absolute_segment)):\n",
    "                continue\n",
    "                \n",
    "            absolute_segment[:, 0] *= original_w\n",
    "            absolute_segment[:, 1] *= original_h\n",
    "\n",
    "            points = absolute_segment.reshape((-1, 1, 2)).astype(np.int32)\n",
    "            \n",
    "            # Validate points\n",
    "            if len(points) < 3 or points.shape[1] != 1 or points.shape[2] != 2:\n",
    "                continue\n",
    "                \n",
    "            # Clamp points to image boundaries\n",
    "            points[:, 0, 0] = np.clip(points[:, 0, 0], 0, original_w-1)\n",
    "            points[:, 0, 1] = np.clip(points[:, 0, 1], 0, original_h-1)\n",
    "            \n",
    "            mask = np.zeros((original_h, original_w), dtype=np.uint8)\n",
    "            try:\n",
    "                cv2.fillPoly(mask, [points], color=np.random.randint(0, 255))\n",
    "                combined_mask = cv2.bitwise_or(combined_mask, mask)\n",
    "            except cv2.error as e:\n",
    "                print(f\"OpenCV fillPoly error in visualization: {e}. Skipping this segment.\")\n",
    "                continue\n",
    "\n",
    "        plt.imshow(combined_mask, cmap='jet')\n",
    "        plt.title('Combined Mask')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_segment_images(self, img_path: str, target_size: int = 640) -> Tuple[List[np.ndarray], List[str]]:\n",
    "        \"\"\"\n",
    "        Generate standardized segment images.\n",
    "\n",
    "        Output images feature:\n",
    "        - Uniform size (target_size x target_size)\n",
    "        - Centered clothing item\n",
    "        - Item occupies ~80% of image area\n",
    "        - Gray background (128, 128, 128)\n",
    "        - Preserved aspect ratio\n",
    "\n",
    "        Args:\n",
    "            img_path: Path to source image\n",
    "            target_size: Output image dimensions (default 640)\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - List of RGB images (numpy arrays)\n",
    "                - List of clothing class names\n",
    "\n",
    "        Process:\n",
    "            1. Extract segments and create masks\n",
    "            2. Apply masks to original image\n",
    "            3. Calculate bounding box with 10% padding\n",
    "            4. Create RGBA image with transparency\n",
    "            5. Resize with preserved aspect ratio\n",
    "            6. Composite onto gray background\n",
    "        \"\"\"\n",
    "        segments, cloth_names = self.segment_clothes(img_path)\n",
    "        if len(segments) == 0:\n",
    "          return []\n",
    "        image = cv2.imread(img_path)\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        segment_images = []\n",
    "        bg_color = (128, 128, 128)  # Gray background\n",
    "\n",
    "        for segment in segments:\n",
    "            # Validate segment data\n",
    "            if segment is None or len(segment) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Convert normalized coordinates to absolute\n",
    "            absolute_segment = segment.copy()\n",
    "            \n",
    "            # Check for invalid coordinates (NaN, infinity)\n",
    "            if np.any(np.isnan(absolute_segment)) or np.any(np.isinf(absolute_segment)):\n",
    "                continue\n",
    "                \n",
    "            absolute_segment[:, 0] *= w\n",
    "            absolute_segment[:, 1] *= h\n",
    "            points = absolute_segment.reshape((-1, 1, 2)).astype(np.int32)\n",
    "            \n",
    "            # Enhanced validation for OpenCV fillPoly requirements\n",
    "            if len(points) < 3:\n",
    "                continue\n",
    "                \n",
    "            # Validate points shape and data\n",
    "            if points.shape[1] != 1 or points.shape[2] != 2:\n",
    "                continue\n",
    "                \n",
    "            # Check if points contain valid coordinate values\n",
    "            if np.any(points < 0) or np.any(points[:, 0, 0] >= w) or np.any(points[:, 0, 1] >= h):\n",
    "                # Clamp points to image boundaries\n",
    "                points[:, 0, 0] = np.clip(points[:, 0, 0], 0, w-1)\n",
    "                points[:, 0, 1] = np.clip(points[:, 0, 1], 0, h-1)\n",
    "            \n",
    "            # Create binary mask\n",
    "            mask = np.zeros((h, w), dtype=np.uint8)\n",
    "            try:\n",
    "                cv2.fillPoly(mask, [points], 255)\n",
    "            except cv2.error as e:\n",
    "                print(f\"OpenCV fillPoly error: {e}. Skipping this segment.\")\n",
    "                continue\n",
    "\n",
    "            # Apply mask to original image\n",
    "            masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "            # Get bounding coordinates\n",
    "            coords = np.where(mask > 0)\n",
    "            y_min, y_max = np.min(coords[0]), np.max(coords[0])\n",
    "            x_min, x_max = np.min(coords[1]), np.max(coords[1])\n",
    "\n",
    "            # Add 10% padding\n",
    "            padding = 0.1\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            x_min_pad = max(0, int(x_min - padding * width))\n",
    "            x_max_pad = min(w, int(x_max + padding * width))\n",
    "            y_min_pad = max(0, int(y_min - padding * height))\n",
    "            y_max_pad = min(h, int(y_max + padding * height))\n",
    "\n",
    "            # Crop image and mask\n",
    "            cropped = masked_image[y_min_pad:y_max_pad, x_min_pad:x_max_pad]\n",
    "            cropped_mask = mask[y_min_pad:y_max_pad, x_min_pad:x_max_pad]\n",
    "\n",
    "            # Create RGBA image\n",
    "            rgba = cv2.cvtColor(cropped, cv2.COLOR_RGB2RGBA)\n",
    "            rgba[:, :, 3] = cropped_mask  # Set alpha channel\n",
    "\n",
    "            # Calculate proportional scaling\n",
    "            scale_factor = 0.8 * target_size / max(rgba.shape[0], rgba.shape[1])\n",
    "            new_width = int(rgba.shape[1] * scale_factor)\n",
    "            new_height = int(rgba.shape[0] * scale_factor)\n",
    "            resized = cv2.resize(rgba, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Create background canvas\n",
    "            result_img = np.zeros((target_size, target_size, 4), dtype=np.uint8)\n",
    "            result_img[:, :, :3] = bg_color\n",
    "            result_img[:, :, 3] = 255  # Opaque background\n",
    "\n",
    "            # Calculate centering position\n",
    "            x_offset = (target_size - new_width) // 2\n",
    "            y_offset = (target_size - new_height) // 2\n",
    "\n",
    "            # Alpha compositing\n",
    "            alpha_s = resized[:, :, 3] / 255.0\n",
    "            alpha_l = 1.0 - alpha_s\n",
    "\n",
    "            for c in range(3):\n",
    "                result_img[y_offset:y_offset+new_height, x_offset:x_offset+new_width, c] = (\n",
    "                    alpha_s * resized[:, :, c] +\n",
    "                    alpha_l * result_img[y_offset:y_offset+new_height, x_offset:x_offset+new_width, c]\n",
    "                )\n",
    "\n",
    "            # Convert to RGB\n",
    "            segment_img = cv2.cvtColor(result_img, cv2.COLOR_RGBA2RGB)\n",
    "            segment_images.append(segment_img)\n",
    "\n",
    "        return ( segment_images, cloth_names )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15d11b",
   "metadata": {
    "id": "II4mpvAVaWeU",
    "outputId": "516c1c4e-d861-4e9d-cdb6-a9edfc98f660",
    "papermill": {
     "duration": 11.161977,
     "end_time": "2025-06-27T14:04:18.451435",
     "exception": false,
     "start_time": "2025-06-27T14:04:07.289458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedder = ClipEmbedder(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "segmentation_model = FashionSegmentationModel(\"/kaggle/input/yolo-detect-clothes-model/pytorch/default/1/best.pt\", 'sam_b.pt',\n",
    "                                              \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2e7ed",
   "metadata": {
    "id": "76_isrzb8awN",
    "papermill": {
     "duration": 0.034819,
     "end_time": "2025-06-27T14:04:18.512577",
     "exception": false,
     "start_time": "2025-06-27T14:04:18.477758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_outfits_to_clothes(embedder: ClipEmbedder,\n",
    "                             segmentation_model: FashionSegmentationModel,\n",
    "                             outfit_dir: str, \n",
    "                             output_dir: str):\n",
    "  completed_idxs = range(0, 11485)\n",
    "  for outfit_number, outfit_name in enumerate(os.listdir(outfit_dir), 1):\n",
    "\n",
    "    outfit_path = f\"{outfit_dir}/{outfit_name}\"\n",
    "    outfit_idx = outfit_name[6:-4]\n",
    "    if outfit_idx in completed_idxs:\n",
    "        continue\n",
    "    try:\n",
    "      result = segmentation_model.get_segment_images(outfit_path)\n",
    "      \n",
    "      # Handle case where get_segment_images returns empty list due to invalid segments\n",
    "      if not result or len(result) == 0:\n",
    "        continue\n",
    "        \n",
    "      segmented_clothes, cloth_names = result\n",
    "\n",
    "      if len(segmented_clothes) == 0:\n",
    "        continue\n",
    "\n",
    "      clothes_embeddings = embedder.get_image_embeddings(segmented_clothes)\n",
    "\n",
    "      # normalize embeddings\n",
    "      norms = np.linalg.norm(clothes_embeddings, axis=1, keepdims=True)\n",
    "      normalized_embeddings = clothes_embeddings / norms\n",
    "\n",
    "      # calculate similarity matrix\n",
    "      similarity_matrix = np.dot(normalized_embeddings, normalized_embeddings.T)\n",
    "\n",
    "      # check on similar clothes in one outfit, remove by threshold\n",
    "      cleaned_clothes = [(segmented_clothes[0], cloth_names[0])]\n",
    "      for cloth_idx in range(1, len(segmented_clothes)):\n",
    "        max_similarity = np.max(similarity_matrix[cloth_idx, :cloth_idx])\n",
    "        if max_similarity < 0.95:\n",
    "          cleaned_clothes.append((segmented_clothes[cloth_idx], cloth_names[cloth_idx]))\n",
    "\n",
    "      # save clothes in files\n",
    "      for cloth_img, cloth_class in cleaned_clothes:\n",
    "        cloth_name = f\"cloth{outfit_idx}_{cloth_class}.jpg\"\n",
    "        cloth_path = f\"{output_dir}/{cloth_name}\"\n",
    "        cv2.imwrite(cloth_path, cloth_img)\n",
    "\n",
    "    except ValueError as error:\n",
    "      print(f\"Outfit {outfit_name} is skipped due to ValueError: {error}\")\n",
    "    except Exception as error:\n",
    "      print(f\"Outfit {outfit_name} is skipped due to unexpected error: {error}\")\n",
    "\n",
    "    if outfit_number % 50 == 0:\n",
    "      print(f\"{outfit_number} outfits are already processes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b85c25",
   "metadata": {
    "papermill": {
     "duration": 0.240948,
     "end_time": "2025-06-27T14:04:18.779123",
     "exception": false,
     "start_time": "2025-06-27T14:04:18.538175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir clothes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8cbe8",
   "metadata": {
    "id": "KtltqvRgIM1T",
    "outputId": "7d06b52a-f596-4a8b-f800-59936510d2df",
    "papermill": {
     "duration": 8753.462773,
     "end_time": "2025-06-27T16:30:12.268352",
     "exception": false,
     "start_time": "2025-06-27T14:04:18.805579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_outfits_to_clothes(embedder, segmentation_model, '/kaggle/input/outfit-predict-25k-outfits-dataset/extracted_images','/kaggle/working/clothes' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102d44f",
   "metadata": {
    "id": "iY1rSzkS61uc",
    "outputId": "1995984c-cebb-488f-a162-82ddecfc6182",
    "papermill": {
     "duration": 67.007593,
     "end_time": "2025-06-27T16:31:22.517192",
     "exception": false,
     "start_time": "2025-06-27T16:30:15.509599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Путь к папке с изображениями\n",
    "input_folder = '/kaggle/working/clothes/'\n",
    "# Название будущего архива\n",
    "output_zip = '/kaggle/working/clothes.zip'\n",
    "\n",
    "# Создаем ZIP-архив\n",
    "with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, start=input_folder)\n",
    "            zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"Архив создан: {output_zip}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7754170,
     "sourceId": 12302265,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 387057,
     "modelInstanceId": 366163,
     "sourceId": 451329,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8951.049846,
   "end_time": "2025-06-27T16:31:33.992131",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-27T14:02:22.942285",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
