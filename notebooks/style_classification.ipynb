{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from typing import List, Union, Optional, Tuple\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionClipEncoder:\n",
    "    def __init__(self,\n",
    "                 model_name: str = \"patrickjohncyh/fashion-clip\",\n",
    "                 device: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the FashionCLIP encoder.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model identifier\n",
    "            device: Optional device override ('cuda', 'cpu', or None for auto-detection)\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        if not self.device:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode_images(\n",
    "        self,\n",
    "        images: List[Union[str, Image.Image]],\n",
    "        batch_size: int = 32,\n",
    "        verbose: bool = False,\n",
    "        normalize: bool = True\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes images in batches.\n",
    "        \n",
    "        Args:\n",
    "            images: List of image paths or PIL Images\n",
    "            batch_size: Number of images to process simultaneously\n",
    "            verbose: Whether to print progress\n",
    "            normalize: Whether to normalize embeddings to unit vectors\n",
    "            \n",
    "        Returns:\n",
    "            Numpy array of all image embeddings (len(images), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not isinstance(images, list):\n",
    "            raise ValueError(\"Input must be a list of images\")\n",
    "\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i + batch_size]\n",
    "            if verbose:\n",
    "                print(f\"Processing image batch {i//batch_size + 1}/{(len(images)-1)//batch_size + 1}\")\n",
    "\n",
    "            loaded_images = []\n",
    "            for img in batch:\n",
    "                if isinstance(img, str):\n",
    "                    loaded_images.append(Image.open(img))\n",
    "                else:\n",
    "                    loaded_images.append(img)\n",
    "\n",
    "            inputs = self.processor(\n",
    "                images=loaded_images,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model.get_image_features(**inputs)\n",
    "                if normalize:\n",
    "                    embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(all_embeddings)\n",
    "\n",
    "    def encode_texts(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        batch_size: int = 128,\n",
    "        verbose: bool = False,\n",
    "        normalize: bool = True\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes texts in batches.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to encode\n",
    "            batch_size: Number of texts to process simultaneously\n",
    "            verbose: Whether to print progress\n",
    "            normalize: Whether to normalize embeddings to unit vectors\n",
    "            \n",
    "        Returns:\n",
    "            Numpy array of all text embeddings (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            raise ValueError(\"Input must be a list of text strings\")\n",
    "\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            if verbose:\n",
    "                print(f\"Processing text batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n",
    "\n",
    "            inputs = self.processor(\n",
    "                text=batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=77\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model.get_text_features(**inputs)\n",
    "                if normalize:\n",
    "                    embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_set() -> Tuple[List[str], List[str]]:\n",
    "    base_dir = \"/kaggle/input/fashion-styles-3/fashion_styles_3\"\n",
    "    image_paths = []\n",
    "    true_labels = []\n",
    "    for style_dir in os.listdir(base_dir):\n",
    "        style_path = os.path.join(base_dir, style_dir)\n",
    "        for img_name in os.listdir(style_path):\n",
    "            img_path = os.path.join(style_path, img_name)\n",
    "            image_paths.append(img_path)\n",
    "            true_labels.append(style_dir)\n",
    "    return image_paths, true_labels\n",
    "\n",
    "def get_style_descriptions() -> dict:\n",
    "    return {\n",
    "        \"formal\": \"business formal, sharply tailored suit, polished\",\n",
    "        \"streetwear\": \"streetwear, urban casual, relax\",\n",
    "        \"minimalist\": \"minimal, clean, monochrome, high‑quality, neutral tones, sophisticated\",\n",
    "        \"athleisure\": \"athleisure, sporty outfit\"\n",
    "    }\n",
    "\n",
    "def get_predictions(encoder: FashionClipEncoder,\n",
    "                    image_paths: List[str],\n",
    "                    labels_desc: dict,\n",
    "                    threshold: float = 0.2) -> List[str]:\n",
    "    label_items = labels_desc.items()\n",
    "    labels = [item[0] for item in label_items]\n",
    "    labels.append(\"other\")\n",
    "    descriptions = [item[1] for item in label_items]\n",
    "    text_embs = encoder.encode_texts(descriptions, batch_size=64, verbose=True)\n",
    "    image_embs = encoder.encode_images(image_paths, batch_size=64, verbose=True)\n",
    "    sim_matrix = image_embs @ text_embs.T\n",
    "    predictions, confidence = np.argmax(sim_matrix, axis=1), np.max(sim_matrix, axis=1)\n",
    "    predictions = np.where(confidence >= threshold, predictions, len(labels)-1)\n",
    "    pred_labels = []\n",
    "    for label_idx in predictions:\n",
    "        pred_labels.append(labels[label_idx])\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = FashionClipEncoder()\n",
    "image_paths, true_labels = get_test_set()\n",
    "style_descriptions = get_style_descriptions()\n",
    "pred_labels = get_predictions(encoder, image_paths, style_descriptions)\n",
    "labels = list(style_descriptions.keys()) + [\"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Отчёт о классификации:\")\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Общая точность (Accuracy): {accuracy_score(true_labels, pred_labels):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Cross-tab отчет:\")\n",
    "cross_tab = pd.crosstab(pd.Series(true_labels, name='Истинный стиль'),\n",
    "                        pd.Series(pred_labels, name='Предсказанный стиль'),\n",
    "                        margins=True)\n",
    "print(cross_tab)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7881743,
     "sourceId": 12489943,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
