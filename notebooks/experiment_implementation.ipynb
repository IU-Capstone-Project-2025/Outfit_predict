{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import PointStruct\n",
    "from sklearn.metrics import ndcg_score\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_FILE = \"dino_small_embeddings.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings_txt(path: str, outfits: bool = True):\n",
    "    \"\"\"\n",
    "    Reads embeddings from a .txt file into a Python list of dictionaries.\n",
    "    There are two modes for parsing the data: one for outfit embeddings\n",
    "    (which include `outfit_id` and `cloth_id`), and another for golden_set\n",
    "    embeddings (which only have `cloth_id`). This separation is necessary\n",
    "    due to differences in the file format's first column (identifier).\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the embeddings `.txt` file.\n",
    "                    Expected format for `outfits=True`: \"cloth<outfit_id>_<cloth_id> <embedding_values>...\"\n",
    "                    Expected format for `outfits=False`: \"<cloth_id>.jpg <embedding_values>...\"\n",
    "        outfits (bool): If True, the function expects and parses the 'outfits' format,\n",
    "                        extracting both `outfit_id` and `cloth_id`.\n",
    "                        If False, it expects the 'golden_set' format, extracting only `cloth_id`.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, where each dictionary represents an embedding entry.\n",
    "                    Each dictionary will contain:\n",
    "                    - \"outfit_id\" (str): Extracted outfit ID (only if `outfits` is True).\n",
    "                    - \"cloth_id\" (str): Extracted cloth ID.\n",
    "                    - \"embeddings\" (list[float]): The numerical embedding vector.\n",
    "    \"\"\"\n",
    "    embeddings_list = [] # Initialize an empty list to store the parsed embedding dictionaries\n",
    "\n",
    "    if outfits:\n",
    "        # Mode for reading outfit embeddings (expected format: \"cloth<outfit_id>_<cloth_id> ...\")\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = f.readlines() # Read all lines from the file\n",
    "            # Strip whitespace from each line and split by space.\n",
    "            # This separates the identifier string from the embedding values.\n",
    "            lines = [line.strip().split(\" \") for line in lines]\n",
    "            \n",
    "            # Iterate through each processed line (which is now a list of strings)\n",
    "            for line in lines:\n",
    "                # Extract outfit_id using regex: looks for digits after \"cloth\"\n",
    "                # Example: \"cloth123_abc\" -> outfit_id \"123\"\n",
    "                outfit_id_match = re.search(r\"cloth(\\d+)\", line[0])\n",
    "                outfit_id = outfit_id_match[1] if outfit_id_match else None\n",
    "\n",
    "                # Extract cloth_id using regex: looks for alphanumeric characters after \"cloth<digits>_\"\n",
    "                # Example: \"cloth123_abc\" -> cloth_id \"abc\"\n",
    "                cloth_id_match = re.search(r\"cloth\\d+_([\\d\\w]+)\", line[0])\n",
    "                cloth_id = cloth_id_match[1] if cloth_id_match else None\n",
    "                \n",
    "                # Append the parsed data as a dictionary to the list\n",
    "                embeddings_list.append({\n",
    "                    \"outfit_id\" : outfit_id,\n",
    "                    \"cloth_id\" : cloth_id,\n",
    "                    \"embeddings\" : [float(val) for val in line[1:]] # Convert all subsequent values to floats for the embedding vector\n",
    "                })\n",
    "        return embeddings_list\n",
    "    else:\n",
    "        # Mode for reading golden_set embeddings (expected format: \"<cloth_id>.jpg ...\")\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = f.readlines() # Read all lines from the file\n",
    "            # Strip whitespace from each line and split by space.\n",
    "            lines = [line.strip().split(\" \") for line in lines]\n",
    "        \n",
    "            # Iterate through each processed line\n",
    "            for line in lines:\n",
    "                # Extract the name (which serves as cloth_id) by removing the \".jpg\" extension\n",
    "                # Example: \"image123.jpg\" -> name \"image123\"\n",
    "                name = line[0][:-4] \n",
    "                \n",
    "                # Append the parsed data as a dictionary to the list\n",
    "                embeddings_list.append({\n",
    "                    \"cloth_id\" : name,\n",
    "                    \"embeddings\" : [float(val) for val in line[1:]] # Convert embedding values to floats\n",
    "                })\n",
    "        return embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = read_embeddings_txt(EMBEDDINGS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = len(embeddings_dict[0][\"embeddings\"])\n",
    "vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Connecting to Qdrant Client & Initial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_CLOUD_URL = \"https://b0da717c-1acf-4983-8bda-2c5214327161.eu-west-2-0.aws.cloud.qdrant.io:6333\"\n",
    "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.aaqOk2k8NhxTemd3DPe8jwFaeYv4Xb6CX3CH2IZ40ts\"\n",
    "COLLECTION_NAME = \"outfit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\n",
    "    url=\"http://localhost:6333\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hnsw_params = models.HnswConfigDiff(\n",
    "    m = 4,\n",
    "    ef_construct = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.collection_exists(collection_name = COLLECTION_NAME):\n",
    "    client.delete_collection(collection_name = COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(\n",
    "    collection_name = COLLECTION_NAME,\n",
    "        vectors_config = models.VectorParams(\n",
    "            size = vector_size,\n",
    "            distance = models.Distance.COSINE\n",
    "        ),\n",
    "        hnsw_config = models.HnswConfigDiff(\n",
    "            m = 8,\n",
    "            ef_construct = 32\n",
    "        ),\n",
    "        quantization_config=models.ScalarQuantization(\n",
    "            scalar=models.ScalarQuantizationConfig(\n",
    "                type=models.ScalarType.INT8,\n",
    "                quantile=0.95,\n",
    "                always_ram=True,\n",
    "            ),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_collection(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_embeddings_to_database(embeddings_dict, max_items: int = None, batch: int = 128):\n",
    "    \"\"\"\n",
    "    Uploads a list of embeddings (points) to a specified Qdrant collection in batches.\n",
    "    It provides real-time progress and estimated time remaining during the upload process.\n",
    "\n",
    "    Args:\n",
    "        embeddings_dict (list[dict]): A list of dictionaries, where each dictionary\n",
    "                                     contains at least \"embeddings\" (the vector)\n",
    "                                     and \"outfit_id\" (for the payload).\n",
    "                                     Expected format: [{\"outfit_id\": \"...\", \"embeddings\": [...]}, ...]\n",
    "        max_items (int, optional): The maximum number of embeddings to upload from the list.\n",
    "                                   If None, all embeddings in `embeddings_dict` will be uploaded.\n",
    "                                   Defaults to None.\n",
    "        batch (int, optional): The number of embeddings to include in each batch upload request\n",
    "                               to Qdrant. Larger batches can be more efficient but consume\n",
    "                               more memory temporarily. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        None: The function performs the upload operation and prints progress.\n",
    "              It does not return any value.\n",
    "    \"\"\"\n",
    "    # Determine the total number of items to upload.\n",
    "    # If max_items is not specified, upload all available embeddings.\n",
    "    if max_items is None:\n",
    "        max_items = len(embeddings_dict)\n",
    "    \n",
    "    # Initialize a list to store the time taken for each upload cycle (batch).\n",
    "    # This is used to calculate the estimated time remaining.\n",
    "    time_for_one_cycle = []\n",
    "    \n",
    "    # Calculate the total number of upload cycles (batches) needed.\n",
    "    # Note: Integer division might truncate, but the range loop handles this.\n",
    "    number_of_cycles = max_items / batch \n",
    "    \n",
    "    # Iterate through the embeddings_dict in steps of 'batch' size.\n",
    "    # 'i' represents the starting index of the current batch.\n",
    "    for i in range(0, max_items, batch):\n",
    "        t1 = time.time() # Record the start time of the current batch upload\n",
    "        \n",
    "        # Prepare the list of PointStruct objects for the current batch.\n",
    "        # Each PointStruct requires a unique 'id', the 'vector' (embedding),\n",
    "        # and an optional 'payload' (metadata).\n",
    "        points_to_upsert = [\n",
    "            PointStruct(\n",
    "                # Generate a unique UUID for each point's ID.\n",
    "                # Converting to string is necessary as Qdrant IDs are strings or integers.\n",
    "                id=str(uuid.uuid4()), \n",
    "                # Assign the embedding vector to the 'vector' field.\n",
    "                vector=item[\"embeddings\"],\n",
    "                # Store relevant metadata in the 'payload'.\n",
    "                # Here, 'outfit_id' is stored for later retrieval/filtering.\n",
    "                payload={\n",
    "                    \"outfit_id\": item[\"outfit_id\"]\n",
    "                }\n",
    "            )\n",
    "            # Slice the embeddings_dict to get the current batch of items.\n",
    "            # enumerate is used here but 'i' is the loop variable, consider if this 'i' is truly needed.\n",
    "            # The enumerate `i` in the list comprehension is actually local to the list comprehension and unused.\n",
    "            # It should just be `for item in embeddings_dict[i:i + batch]`.\n",
    "            for item in embeddings_dict[i:i + batch] \n",
    "        ]\n",
    "        \n",
    "        # Send the batch of points to Qdrant for upsertion (insert or update).\n",
    "        # 'COLLECTION_NAME' must be defined and point to your target Qdrant collection.\n",
    "        # 'wait=True' ensures that the function call blocks until Qdrant confirms the operation\n",
    "        # has been completed for this batch, which is useful for reliable uploads and timing.\n",
    "        operation_info = client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            wait=True,\n",
    "            points=points_to_upsert\n",
    "        )\n",
    "        \n",
    "        t2 = time.time() # Record the end time of the current batch upload\n",
    "        dt = round(t2 - t1, 2) # Calculate the elapsed time for this batch\n",
    "        time_for_one_cycle.append(dt) # Store the time for calculating estimated remaining time\n",
    "        \n",
    "        # Print progress and estimated time remaining.\n",
    "        # The '\\r' at the end ensures the line is overwritten in the console,\n",
    "        # creating a dynamic progress bar.\n",
    "        # Calculation for estimated time: (remaining_items / batch_size) * average_time_per_batch\n",
    "        print(f\"Progress: {100*(i / max_items):.3f}%;   \"\n",
    "              f\"Estimated time: {((max_items - i) / batch) * np.mean(time_for_one_cycle):.3f} seconds\\t\",\n",
    "              end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_embeddings_to_database(embeddings_dict, batch = 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_payload_index(\n",
    "    collection_name= COLLECTION_NAME,\n",
    "    field_name = \"outfit_id\",\n",
    "    field_schema = models.PayloadSchemaType.KEYWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    collection_info = client.get_collection(collection_name = COLLECTION_NAME)\n",
    "    if collection_info.status == models.CollectionStatus.GREEN:\n",
    "        # Collection status is green, which means the indexing is finished\n",
    "        print('Indexing finished')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Search similar Outfits Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar(golden_set_embeddings_path: str,\n",
    "                   client: QdrantClient,\n",
    "                   search_params: models.SearchParams,\n",
    "                   score_threshold: float = 0.7,\n",
    "                   collection_name: str = COLLECTION_NAME,\n",
    "                   num_of_outfits_to_return: int = 5):\n",
    "    \"\"\"\n",
    "    Performs a complex similarity search to find the most relevant \"outfits\"\n",
    "    from a Qdrant collection based on a \"golden set\" of wardrobe embeddings.\n",
    "    \n",
    "    The function operates in several stages:\n",
    "    1. Loads the \"golden set\" (wardrobe) embeddings.\n",
    "    2. For each item in the wardrobe, queries Qdrant to find similar individual clothing items.\n",
    "    3. Aggregates the `outfit_id`s from these similar individual items to identify candidate outfits.\n",
    "    4. For each candidate outfit, retrieves all its associated clothing items from Qdrant.\n",
    "    5. Calculates a \"total_score\" for each candidate outfit by finding the best match\n",
    "       between its items and the wardrobe items, summing their similarity scores.\n",
    "    6. Filters out outlier clothing items within an outfit that might skew scores.\n",
    "    7. Returns the top `num_of_outfits_to_return` ranked outfits.\n",
    "\n",
    "    Args:\n",
    "        golden_set_embeddings_path (str): Path to the `.txt` file containing the\n",
    "                                          embeddings of the \"golden set\" (wardrobe items).\n",
    "        client (qdrant_client.QdrantClient): An initialized Qdrant client instance.\n",
    "        search_params (models.SearchParams): Parameters for the Qdrant vector search,\n",
    "                                           e.g., `hnsw_ef` for HNSW algorithm.\n",
    "                                           For the \"control\" search, this should typically\n",
    "                                           be set with `exact=True` to represent ground truth.\n",
    "        score_threshold (float, optional): A minimum similarity score for individual\n",
    "                                           clothing items to be considered \"similar\"\n",
    "                                           when querying Qdrant. Defaults to 0.7.\n",
    "        collection_name (str, optional): The name of the Qdrant collection where outfit\n",
    "                                         clothing items are stored. Defaults to `COLLECTION_NAME`.\n",
    "        num_of_outfits_to_return (int, optional): The maximum number of top-ranked outfits\n",
    "                                                  to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, where each dictionary represents a ranked outfit.\n",
    "                    Each outfit dictionary contains:\n",
    "                    - \"outfit_id\" (str): The unique identifier of the outfit.\n",
    "                    - \"total_score\" (float): The aggregated similarity score for the outfit.\n",
    "                    - \"matches\" (list[dict]): Details of the best matching wardrobe-to-outfit-item pairs.\n",
    "                    Returns an empty list if no candidate outfits are found or if an error occurs.\n",
    "    \"\"\"\n",
    "    # Load the \"golden set\" (wardrobe) embeddings from the specified text file.\n",
    "    # 'outfits=False' tells read_embeddings_txt to parse it in the golden_set format.\n",
    "    wardrobe_embeddings = read_embeddings_txt(golden_set_embeddings_path, outfits=False)\n",
    "    \n",
    "    # Extract just the embedding vectors into a NumPy array for efficient matrix operations later.\n",
    "    wardrobe_embeddings_vectors = np.array([item[\"embeddings\"] for item in wardrobe_embeddings])\n",
    "\n",
    "    # Initialize a set to store unique outfit IDs found during the initial broad search.\n",
    "    # A set is used to automatically handle duplicates.\n",
    "    candidate_outfits_ids = set()\n",
    "\n",
    "    # Step 1: Initial broad search to find candidate outfits.\n",
    "    # For each embedding in the wardrobe (golden set):\n",
    "    for i in range(len(wardrobe_embeddings)):\n",
    "        # Use the current wardrobe item's embedding as the query vector.\n",
    "        query_vector = wardrobe_embeddings[i][\"embeddings\"]\n",
    "        \n",
    "        # Query Qdrant to find clothing items similar to the current wardrobe item.\n",
    "        # 'limit=50' retrieves up to 50 similar points for each wardrobe item query.\n",
    "        # 'search_params' (e.g., hnsw_ef) controls the search algorithm's behavior.\n",
    "        # 'score_threshold' filters out very dissimilar individual clothing items.\n",
    "        similar_points = client.query_points(\n",
    "            collection_name=collection_name, # Query the collection of all individual clothing items\n",
    "            query=query_vector,\n",
    "            limit=50,\n",
    "            search_params=search_params, \n",
    "            score_threshold=score_threshold\n",
    "        ).points\n",
    "        \n",
    "        # From the similar individual clothing items found, extract their associated outfit IDs.\n",
    "        for point in similar_points:\n",
    "            # Ensure the payload contains 'outfit_id' before attempting to access it.\n",
    "            if \"outfit_id\" in point.payload:\n",
    "                # Add the outfit_id to the set of candidates.\n",
    "                candidate_outfits_ids.add(point.payload[\"outfit_id\"])\n",
    "                \n",
    "    # If no candidate outfits were identified after iterating through all wardrobe items,\n",
    "    # print a message and return an empty list.\n",
    "    if not candidate_outfits_ids:\n",
    "        print(\"No candidate outfits were found.\")\n",
    "        return []\n",
    "    \n",
    "    # Initialize a list to store dictionaries of ranked outfits, including their scores and matches.\n",
    "    ranked_outfits = []\n",
    "    \n",
    "    # Step 2: Refine and score each candidate outfit.\n",
    "    # For each unique outfit_id identified in the previous step:\n",
    "    for outfit_id in candidate_outfits_ids:\n",
    "        # Retrieve all individual clothing items that belong to the current outfit.\n",
    "        # 'scroll' is used to efficiently retrieve points that match a specific filter.\n",
    "        # The filter is set to find all points where 'outfit_id' matches the current outfit_id.\n",
    "        records, next_offset = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            scroll_filter=models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(\n",
    "                        key=\"outfit_id\",\n",
    "                        match=models.MatchValue(value=outfit_id)\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            limit=100, # Retrieve up to 100 items per outfit. Adjust if outfits can be larger.\n",
    "            with_payload=True, # Ensure payload (metadata) is returned with each record.\n",
    "            with_vectors=True # Ensure embedding vectors are returned with each record.\n",
    "        )\n",
    "        \n",
    "        # If no records are found for an outfit ID (shouldn't happen if outfit_id came from a valid point),\n",
    "        # or if there's an issue, print an error and return.\n",
    "        if not records:\n",
    "            print(f\"No clothes for outfit_id '{outfit_id}' were found, some error occurred.\")\n",
    "            return []\n",
    "        \n",
    "        # Extract embeddings and IDs of the clothing items belonging to the current outfit.\n",
    "        outfit_item_embeddings = np.array([record.vector for record in records])\n",
    "        outfit_item_ids = [record.id for record in records]\n",
    "\n",
    "        # An outfit must consist of at least two items to be considered valid.\n",
    "        # This prevents single-item \"outfits\" from being scored.\n",
    "        if len(outfit_item_ids) < 2:\n",
    "            continue # Skip this outfit if it's too small.\n",
    "            \n",
    "        # Calculate the similarity matrix between all wardrobe item embeddings and all\n",
    "        # items in the current candidate outfit.\n",
    "        # The result is a matrix where rows are wardrobe items and columns are outfit items.\n",
    "        # Each cell (i, j) contains the dot product (similarity) between wardrobe_embeddings_vectors[i]\n",
    "        # and outfit_item_embeddings[j].\n",
    "        similarity_matrix = np.dot(wardrobe_embeddings_vectors, outfit_item_embeddings.T)\n",
    "        \n",
    "        # For each outfit item, find the wardrobe item that matches it best.\n",
    "        # `best_matches_indices` stores the index of the best-matching wardrobe item for each outfit item.\n",
    "        # `best_matches_scores` stores the similarity score of that best match.\n",
    "        best_matches_indices = np.argmax(similarity_matrix, axis=0)\n",
    "        best_matches_scores = np.max(similarity_matrix, axis=0)\n",
    "        \n",
    "        # Initialize a list to store details of the matched items within the outfit.\n",
    "        matches = []\n",
    "        \n",
    "        # Calculate the mean of the best match scores for the current outfit.\n",
    "        best_matches_mean = np.mean(best_matches_scores)\n",
    "        \n",
    "        # Iterate through each clothing item in the current outfit to process its best match.\n",
    "        for i, outfit_item_id in enumerate(outfit_item_ids):\n",
    "            # IMPORTANT CONSIDERATION: Outlier Detection based on similarity score.\n",
    "            # If an outfit item's best match score is significantly lower than the average\n",
    "            # best match score for the entire outfit, it's considered an outlier.\n",
    "            # This helps to filter out items that might be irrelevant or poorly embedded,\n",
    "            # preventing them from negatively impacting the total outfit score.\n",
    "            # The threshold (0.75 * mean) is heuristic and can be tuned.\n",
    "            if best_matches_scores[i] < best_matches_mean * 0.75:\n",
    "                # \"if some item has very low similarity score compared to others items\n",
    "                # then it means it is outlier, and probably will disturb the prediction\"\n",
    "                # This logic is based on the observation that certain item types (like boots)\n",
    "                # might inherently have lower scores, and this threshold aims to filter\n",
    "                # out items that are too dissimilar within the context of the current outfit's matches.\n",
    "                # \"boots usually have low score\" - this note explains a specific domain observation\n",
    "                # justifying the need for such an outlier filter.\n",
    "                continue # Skip this outlier item.\n",
    "            \n",
    "            # Get the index of the best-matching wardrobe item.\n",
    "            wardrobe_idx = int(best_matches_indices[i])\n",
    "            \n",
    "            # Add details of the matched pair to the 'matches' list.\n",
    "            matches.append({\n",
    "                \"wardrobe_image_index\": wardrobe_idx,\n",
    "                \"wardrobe_image_id\": wardrobe_embeddings[wardrobe_idx][\"cloth_id\"],\n",
    "                \"outfit_item_id\": str(outfit_item_id),\n",
    "                \"score\": float(best_matches_scores[i])\n",
    "            })\n",
    "        \n",
    "        # Calculate the total score for the current outfit.\n",
    "        # This is the sum of the best match scores (after potential outlier filtering).\n",
    "        # \"if we take np.mean(), for some reason it will ignore boots in the wardrobe\n",
    "        # so that it will search only for outfits with tshirt and pants\n",
    "        # given the fact that boots always have lower scores (24 vs 35 for other)\n",
    "        # we will always ignore outfits with boots\"\n",
    "        # This note explains why `np.sum` is used instead of `np.mean` for `total_score`.\n",
    "        # Using mean might inadvertently penalize outfits containing items (like boots)\n",
    "        # that naturally have lower, but still acceptable, similarity scores.\n",
    "        # Summing avoids this by favoring outfits with more relevant overall matches.\n",
    "        total_score = np.sum(best_matches_scores)\n",
    "        \n",
    "        # Append the scored outfit to the list of ranked outfits.\n",
    "        ranked_outfits.append({\n",
    "            \"outfit_id\": outfit_id,\n",
    "            \"total_score\": total_score,\n",
    "            \"matches\": matches # Include the details of the best matches\n",
    "        })\n",
    "        \n",
    "    # Sort the outfits by their 'total_score' in descending order\n",
    "    # to get the top-ranked outfits.\n",
    "    ranked_outfits.sort(key=lambda x: x[\"total_score\"], reverse=True)\n",
    "    \n",
    "    # Return only the top 'num_of_outfits_to_return' outfits.\n",
    "    return ranked_outfits[:num_of_outfits_to_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_process_metrics():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    cpu_percent = process.cpu_percent(interval = None)\n",
    "    ram_info = process.memory_info()\n",
    "    ram_used_mb = ram_info.rss / (1024 * 1024)\n",
    "    return {\"cpu_percent\": cpu_percent, \"ram_used_mb\": ram_used_mb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_memory_usage(client: QdrantClient,\n",
    "                             collection_name: str = COLLECTION_NAME):\n",
    "    number_of_vectors = client.get_collection(collection_name = collection_name).points_count\n",
    "    vector_size_vector = client.get_collection(collection_name = collection_name).config.params.vectors.size\n",
    "    memory_size_vector = number_of_vectors * vector_size_vector * 4 * 1.5 \n",
    "    memory_size_mb_vector = memory_size_vector / (1024 * 1024)\n",
    "    \n",
    "    payload_size = number_of_vectors * 56 * 1.5\n",
    "    # 56 stands for bytes occupied by the \"outfit_id\" payload. This is actually approximate\n",
    "    payload_size_mb = payload_size / (1024 * 1024)\n",
    "    \n",
    "    return {\n",
    "        \"vectors_size_mb\": memory_size_mb_vector,\n",
    "        \"payload_size_mb\": payload_size_mb,\n",
    "        \"overall_memory_usage_mb\": memory_size_mb_vector + payload_size_mb\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exact_search_metrics(client: QdrantClient,\n",
    "                             golden_set_embeddings_path: str,\n",
    "                             collection_name: str = COLLECTION_NAME,\n",
    "                             k: int = 5):\n",
    "    client.update_collection(\n",
    "        collection_name = collection_name,\n",
    "        hnsw_config = models.HnswConfigDiff(\n",
    "            m = 0\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    perfmetrics_control_local_begin = get_process_metrics()\n",
    "    search_time_control_begin = time.time()\n",
    "    search_results_control = search_similar(\n",
    "            golden_set_embeddings_path=golden_set_embeddings_path,\n",
    "            client=client,\n",
    "            search_params=models.SearchParams(exact = True, quantization = models.QuantizationSearchParams(ignore = True)), \n",
    "            num_of_outfits_to_return=k\n",
    "    )\n",
    "    search_time_control_end = time.time()\n",
    "    perfmetrics_control_local_end = get_process_metrics()\n",
    "    cached_results = {\n",
    "            \"search_results_control\": search_results_control,\n",
    "            \"perfmetrics_control_local_begin\": perfmetrics_control_local_begin,\n",
    "            \"search_time_control_begin\": search_time_control_begin,\n",
    "            \"search_time_control_end\": search_time_control_end,\n",
    "            \"perfmetrics_control_local_end\": perfmetrics_control_local_end\n",
    "    }\n",
    "        \n",
    "        \n",
    "    return cached_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_for_configuration(client: QdrantClient,\n",
    "                                        golden_set_embeddings_path: str,\n",
    "                                        cached_results,\n",
    "                                        search_params_control: models.SearchParams,\n",
    "                                        search_params_test: models.SearchParams,\n",
    "                                        k: int = 5,\n",
    "                                        ):\n",
    "    \"\"\"\n",
    "    Measures search latency, accuracy metrics (Precision, Recall, NDCG), and local \n",
    "    resource usage for two different search configurations.\n",
    "\n",
    "    Args:\n",
    "        client (QdrantClient): The Qdrant client instance.\n",
    "        golden_set_embeddings_path (str): Path to the embeddings for the queries.\n",
    "        search_params_control (models.SearchParams): Search parameters for the ground truth search (should be exact=True).\n",
    "        search_params_test (models.SearchParams): Search parameters for the HNSW test search.\n",
    "        k (int): The number of top results to consider (k in @k).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing all calculated performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # --- Execute Control Search (Ground Truth) ---\n",
    "    perfmetrics_control_local_begin = cached_results[\"perfmetrics_control_local_begin\"]\n",
    "    search_time_control_begin = cached_results[\"search_time_control_begin\"]\n",
    "    search_results_control = cached_results[\"search_results_control\"]\n",
    "    search_time_control_end = cached_results[\"search_time_control_end\"]\n",
    "    perfmetrics_control_local_end = cached_results[\"perfmetrics_control_local_end\"]\n",
    "\n",
    "    \n",
    "    # --- Execute Test Search (HNSW) ---\n",
    "    perfmetrics_test_local_begin = get_process_metrics()\n",
    "    search_time_test_begin = time.time()\n",
    "    search_results_test = search_similar(\n",
    "        golden_set_embeddings_path=golden_set_embeddings_path,\n",
    "        client=client,\n",
    "        search_params=search_params_test,\n",
    "        num_of_outfits_to_return=k\n",
    "    )\n",
    "    search_time_test_end = time.time()\n",
    "    perfmetrics_test_local_end = get_process_metrics()\n",
    "    \n",
    "    # --- Extract relevant data for metric calculation ---\n",
    "    # IDs from the ground truth search results\n",
    "    ground_truth_outfit_ids = np.array([result[\"outfit_id\"] for result in search_results_control])\n",
    "    # IDs from the HNSW test search results\n",
    "    test_outfit_ids = np.array([result[\"outfit_id\"] for result in search_results_test])\n",
    "    # Scores from the HNSW test search results\n",
    "    test_scores = np.array([result[\"total_score\"] for result in search_results_test])\n",
    "    \n",
    "    # --- Calculate Accuracy Metrics ---\n",
    "    \n",
    "    # Create a binary relevance vector for the test results based on the ground truth.\n",
    "    # 1 if the test result ID is in the ground truth set, 0 otherwise.\n",
    "    y_true_relevance = np.array([1 if outfit_id in ground_truth_outfit_ids else 0 for outfit_id in test_outfit_ids])\n",
    "    y_true_relevance_2d = y_true_relevance.reshape(1, -1)\n",
    "    \n",
    "    # Reshape the test scores for scikit-learn\n",
    "    test_scores_2d = test_scores.reshape(1, -1)\n",
    "    \n",
    "    # Calculate Precision@k: fraction of retrieved documents that are relevant\n",
    "    precision_at_k = np.sum(y_true_relevance) / k\n",
    "    \n",
    "    # Calculate Recall@k: fraction of relevant documents that are retrieved\n",
    "    # Denominator is the size of the ground truth set (k in this case)\n",
    "    recall_at_k = np.sum(y_true_relevance) / len(ground_truth_outfit_ids)\n",
    "    \n",
    "    # Calculate NDCG@k: quality of the ranked list, penalizing lower-ranked relevant items\n",
    "    ndcg_at_k = ndcg_score(y_true=y_true_relevance_2d, y_score=test_scores_2d, k=k)\n",
    "    \n",
    "    # --- Calculate Time and Resource Metrics ---\n",
    "    time_elapsed_for_control = search_time_control_end - search_time_control_begin\n",
    "    time_elapsed_for_test = search_time_test_end - search_time_test_begin\n",
    "    \n",
    "    avg_cpu_perc_control = np.mean([perfmetrics_control_local_begin[\"cpu_percent\"], perfmetrics_control_local_end[\"cpu_percent\"]])\n",
    "    avg_cpu_perc_test = np.mean([perfmetrics_test_local_begin[\"cpu_percent\"], perfmetrics_test_local_end[\"cpu_percent\"]])\n",
    "    \n",
    "    avg_ram_control = np.mean([perfmetrics_control_local_begin[\"ram_used_mb\"], perfmetrics_control_local_end[\"ram_used_mb\"]])\n",
    "    avg_ram_test = np.mean([perfmetrics_test_local_begin[\"ram_used_mb\"], perfmetrics_test_local_end[\"ram_used_mb\"]])\n",
    "    \n",
    "\n",
    "    # --- Return all metrics in a clear dictionary ---\n",
    "    return {\n",
    "        \"time_elapsed_control_s\": time_elapsed_for_control,\n",
    "        \"time_elapsed_test_s\": time_elapsed_for_test,\n",
    "        \"local_cpu_perc_control_avg\": avg_cpu_perc_control,\n",
    "        \"local_cpu_perc_test_avg\": avg_cpu_perc_test,\n",
    "        \"local_ram_control_mb_avg\": avg_ram_control,\n",
    "        \"local_ram_test_mb_avg\": avg_ram_test,\n",
    "        \"precision_at_k\": precision_at_k,\n",
    "        \"recall_at_k\": recall_at_k,\n",
    "        \"ndcg_at_k\": ndcg_at_k\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_metrics = get_exact_search_metrics(client, \"dino_small_embeddings_men_casual.txt\", COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_parameters(\n",
    "                           client: QdrantClient,\n",
    "                           golden_set_embeddings_path: str,\n",
    "                           df_list,\n",
    "                           df_types: str,\n",
    "                           collection_name: str = COLLECTION_NAME,\n",
    "                           cached_metrics = cached_metrics\n",
    "                           ):\n",
    "    dict_params_list = [df.to_dict(orient='records') for df in df_list]\n",
    "    total_number_of_combinations = sum(df.shape[0] for df in df_list)\n",
    "    all_metrics = []\n",
    "    count_passed = 0\n",
    "    times_went = [0]\n",
    "    print(f\"Starting grid search, total number of combinations: {total_number_of_combinations}\")\n",
    "    for dict_params, df_type in zip(dict_params_list, df_types):\n",
    "        for param in dict_params:\n",
    "            print(f\"<------------------ ITERATION {count_passed + 1} ------------------>\")\n",
    "            t1 = time.time()\n",
    "            print(f\"Progress: {100 * count_passed / total_number_of_combinations:.3f}%; Estimated time: {(total_number_of_combinations - count_passed) * np.mean(times_went):.3f} seconds\")\n",
    "            print(f\"Going through the following parameters: {param}\")\n",
    "            if df_type == 'binary':\n",
    "                client.update_collection(\n",
    "                    collection_name = collection_name,\n",
    "                    hnsw_config = models.HnswConfigDiff(\n",
    "                        m = param[\"m\"],\n",
    "                        ef_construct = param[\"ef_construct\"]\n",
    "                    ),\n",
    "                )\n",
    "                \n",
    "                while True:\n",
    "                    time.sleep(3)\n",
    "                    \n",
    "                    collection_info = client.get_collection(collection_name = COLLECTION_NAME)\n",
    "                    \n",
    "                    if collection_info.status == models.CollectionStatus.GREEN:\n",
    "                        print(\"Indexing finished for these params\")\n",
    "                        break\n",
    "                    \n",
    "                metrics = calculate_metrics_for_configuration(client = client, \n",
    "                        golden_set_embeddings_path = golden_set_embeddings_path,\n",
    "                        cached_results = cached_metrics,\n",
    "                        search_params_control = models.SearchParams(hnsw_ef = param[\"hnsw_ef\"], exact = True,\n",
    "                                    quantization=models.QuantizationSearchParams(\n",
    "                                    ignore = False, rescore = param[\"rescore\"], oversampling = param[\"oversampling\"])),\n",
    "                        search_params_test = models.SearchParams(hnsw_ef = param[\"hnsw_ef\"], exact = False,\n",
    "                                    quantization=models.QuantizationSearchParams(\n",
    "                                    ignore = False, rescore = param[\"rescore\"], oversampling = param[\"oversampling\"])))                     \n",
    "\n",
    "            elif df_type == 'product':\n",
    "                \n",
    "                if param[\"compression\"] == 16:\n",
    "                    client.update_collection(\n",
    "                        collection_name = collection_name,\n",
    "                        hnsw_config = models.HnswConfigDiff(\n",
    "                            m = param[\"m\"], \n",
    "                            ef_construct = param[\"ef_construct\"]\n",
    "                        ),\n",
    "                        quantization_config=models.ProductQuantization(\n",
    "                            product=models.ProductQuantizationConfig(\n",
    "                                compression=models.CompressionRatio.X16,\n",
    "                                always_ram=True,\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "                elif param[\"compression\"] == 32:\n",
    "                    client.update_collection(\n",
    "                        collection_name = collection_name,\n",
    "                        hnsw_config = models.HnswConfigDiff(\n",
    "                            m = param[\"m\"], \n",
    "                            ef_construct = param[\"ef_construct\"]\n",
    "                        ),\n",
    "                        quantization_config=models.ProductQuantization(\n",
    "                            product=models.ProductQuantizationConfig(\n",
    "                                compression=models.CompressionRatio.X32,\n",
    "                                always_ram=True,\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "                elif param[\"compression\"] == 64: \n",
    "                    client.update_collection(\n",
    "                        collection_name = collection_name,\n",
    "                        hnsw_config = models.HnswConfigDiff(\n",
    "                            m = param[\"m\"], \n",
    "                            ef_construct = param[\"ef_construct\"]\n",
    "                        ),\n",
    "                        quantization_config=models.ProductQuantization(\n",
    "                            product=models.ProductQuantizationConfig(\n",
    "                                compression=models.CompressionRatio.X64,\n",
    "                                always_ram=True,\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "                \n",
    "                while True:\n",
    "                    time.sleep(3)\n",
    "                    collection_info = client.get_collection(collection_name = COLLECTION_NAME)\n",
    "                    if collection_info.status == models.CollectionStatus.GREEN:\n",
    "                        print(\"Indexing finished for these params\")\n",
    "                        break\n",
    "                    \n",
    "                metrics = calculate_metrics_for_configuration(client = client, \n",
    "                        golden_set_embeddings_path = golden_set_embeddings_path,\n",
    "                        cached_results = cached_metrics,\n",
    "                        search_params_control = models.SearchParams(hnsw_ef = param[\"hnsw_ef\"], exact = True,\n",
    "                                    quantization=models.QuantizationSearchParams(\n",
    "                                    ignore = False, rescore = param[\"rescore\"], \n",
    "                                    oversampling = param[\"oversampling\"])),\n",
    "                        search_params_test = models.SearchParams(hnsw_ef = param[\"hnsw_ef\"], exact = False,\n",
    "                                    quantization=models.QuantizationSearchParams(\n",
    "                                    ignore = False, rescore = param[\"rescore\"], \n",
    "                                    oversampling = param[\"oversampling\"])))   \n",
    "                \n",
    "            elif df_type == 'scalar':\n",
    "                    \n",
    "                client.update_collection(\n",
    "                    collection_name = collection_name,\n",
    "                    hnsw_config = models.HnswConfigDiff(\n",
    "                            m = param[\"m\"],\n",
    "                            ef_construct = param[\"ef_construct\"]\n",
    "                    ),\n",
    "                    quantization_config=models.ScalarQuantization(\n",
    "                        scalar=models.ScalarQuantizationConfig(\n",
    "                            type=models.ScalarType.INT8,\n",
    "                            quantile=param[\"quantile\"],\n",
    "                            always_ram=True,\n",
    "                        ),\n",
    "                    ),\n",
    "                )\n",
    "                \n",
    "                while True:\n",
    "                    time.sleep(3)\n",
    "                    collection_info = client.get_collection(collection_name = COLLECTION_NAME)\n",
    "                    if collection_info.status == models.CollectionStatus.GREEN:\n",
    "                        print(\"Indexing finished for these params\")\n",
    "                        break\n",
    "                    \n",
    "                metrics = calculate_metrics_for_configuration(client = client, \n",
    "                        golden_set_embeddings_path = golden_set_embeddings_path,\n",
    "                        cached_results = cached_metrics,\n",
    "                        search_params_control = models.SearchParams(hnsw_ef = param[\"hnsw_ef\"], exact = True,\n",
    "                                    quantization=models.QuantizationSearchParams(\n",
    "                                    ignore = False, rescore = param[\"rescore\"], oversampling = param[\"oversampling\"])),\n",
    "                        search_params_test = models.SearchParams(hnsw_ef = param[\"hnsw_ef\"], exact = False,\n",
    "                                    quantization=models.QuantizationSearchParams(\n",
    "                                    ignore = False, rescore = param[\"rescore\"], oversampling = param[\"oversampling\"])))    \n",
    "            \n",
    "            count_passed += 1\n",
    "            all_metrics.append({**param, **metrics})\n",
    "            t2 = time.time()\n",
    "            if count_passed == 1:\n",
    "                    times_went = []\n",
    "            dt = t2 - t1\n",
    "            times_went.append(dt) \n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(all_metrics)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_parameters(client=client,\n",
    "                                 golden_set_embeddings_path=\"dino_small_embeddings_men_casual.txt\",\n",
    "                                 df_list = [\n",
    "                                     pd.read_csv('combinations_product.csv').drop(columns=[\"Unnamed: 0\"]),\n",
    "                                     pd.read_csv('combinations_binary.csv').drop(columns=[\"Unnamed: 0\"]),\n",
    "                                     pd.read_csv('combinations_scalar.csv').drop(columns=[\"Unnamed: 0\"]),\n",
    "                                 ],\n",
    "                                 df_types= ['product', 'binary', 'scalar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(f\"results_{EMBEDDINGS_FILE}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_suitable = results[(results.precision_at_k >= 1.0) & (results.recall_at_k >= 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_suitable_sorted = results_suitable.sort_values(by='time_elapsed_test_s', ascending=True)\n",
    "results_suitable_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.update_collection(\n",
    "    collection_name = COLLECTION_NAME,\n",
    "        hnsw_config = models.HnswConfigDiff(\n",
    "            m = 8,\n",
    "            ef_construct = 32\n",
    "        ),\n",
    "        quantization_config=models.ScalarQuantization(\n",
    "            scalar=models.ScalarQuantizationConfig(\n",
    "                type=models.ScalarType.INT8,\n",
    "                quantile=0.95,\n",
    "                always_ram=True,\n",
    "            ),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_collection(collection_name = COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_demo = search_similar(\"dino_base_embeddings_men_casual.txt\",\n",
    "               client,\n",
    "               search_params = models.SearchParams(hnsw_ef = 16, exact = False,\n",
    "                                    quantization=models.QuantizationSearchParams(\n",
    "                                    ignore = False, rescore = True, oversampling = 1)),\n",
    "               collection_name = COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
