{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075be7b4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 9.058331,
     "end_time": "2025-06-27T21:43:08.592068",
     "exception": false,
     "start_time": "2025-06-27T21:42:59.533737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union, List, Optional\n",
    "from torchvision import transforms\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import torch\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c003b9d",
   "metadata": {
    "papermill": {
     "duration": 25.760021,
     "end_time": "2025-06-27T21:43:34.355100",
     "exception": false,
     "start_time": "2025-06-27T21:43:08.595079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "zip_path = '/kaggle/input/notebook7d00437634/clothes.zip'\n",
    "\n",
    "extract_dir = '/kaggle/working/clothes'\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "    print(f'Files are located in {extract_dir}')\n",
    "\n",
    "extracted_files = os.listdir(extract_dir)\n",
    "print(\"Files\", extracted_files[:10])\n",
    "\n",
    "\n",
    "img_path = os.path.join(extract_dir, extracted_files[0])\n",
    "img = Image.open(img_path)\n",
    "    \n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f'Визуализация: {extracted_files[0]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782bb1f",
   "metadata": {
    "papermill": {
     "duration": 0.019642,
     "end_time": "2025-06-27T21:43:34.380709",
     "exception": false,
     "start_time": "2025-06-27T21:43:34.361067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base = \"/kaggle/working/clothes\"\n",
    "clothes = [f\"{base}/{file}\" for file in extracted_files]\n",
    "clothes[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9da8146f",
   "metadata": {
    "papermill": {
     "duration": 0.005139,
     "end_time": "2025-06-27T21:43:34.391280",
     "exception": false,
     "start_time": "2025-06-27T21:43:34.386141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from typing import Union, List, Optional\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750f01b",
   "metadata": {
    "papermill": {
     "duration": 0.017497,
     "end_time": "2025-06-27T21:43:34.413992",
     "exception": false,
     "start_time": "2025-06-27T21:43:34.396495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DinoV2ImageEncoder:\n",
    "    \"\"\"\n",
    "    A class for encoding images into embeddings\n",
    "    using models from the DINOv2 family.\n",
    "\n",
    "    This class loads a specified DINOv2 model and its corresponding\n",
    "    image transformations. It can process both single images and batches of images.\n",
    "\n",
    "    Attributes:\n",
    "        device (torch.device): The device (CPU or CUDA) on which the model is running.\n",
    "        model (torch.nn.Module): The loaded DINOv2 model.\n",
    "        transform (transforms.Compose): The image transformation pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'dinov2_vitb14', device: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initializes the image encoder.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the DINOv2 model to load.\n",
    "                Available options include: 'dinov2_vits14', 'dinov2_vitb14',\n",
    "                'dinov2_vitl14', 'dinov2_vitg14'.\n",
    "            device (Optional[str]): The device to run the model on ('cuda', 'cpu').\n",
    "                If None, it will auto-detect CUDA availability and use it,\n",
    "                otherwise it will fall back to CPU.\n",
    "        \"\"\"\n",
    "        print(\"Initializing DinoV2ImageEncoder...\")\n",
    "\n",
    "        self.device = self._get_device(device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        print(f\"Loading model '{model_name}'...\")\n",
    "        self.model = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        print(\"Model loaded successfully.\")\n",
    "\n",
    "        # Standard transformations for ViT/DINOv2 models\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "    def _get_device(self, device: Optional[str]) -> torch.device:\n",
    "        \"\"\"Determines the computation device.\"\"\"\n",
    "        if device and torch.cuda.is_available() and device.lower() == 'cuda':\n",
    "            return torch.device(\"cuda\")\n",
    "        elif device and device.lower() == 'cpu':\n",
    "             return torch.device(\"cpu\")\n",
    "        \n",
    "        # Auto-select\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _load_and_preprocess_image(self, image_input: Union[str, Image.Image]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Loads and preprocesses a single image.\n",
    "\n",
    "        Args:\n",
    "            image_input (Union[str, Image.Image]): The path to the image file, a URL, or a PIL.Image object.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The preprocessed image tensor.\n",
    "        \"\"\"\n",
    "        if isinstance(image_input, str):\n",
    "            image = Image.open(image_input)\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        return self.transform(image)\n",
    "\n",
    "    def encode(self, \n",
    "             image_inputs: Union[str, Image.Image, List[Union[str, Image.Image]]], \n",
    "             batch_size: int = 64) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a single image or a batch of images into embeddings using mini-batching.\n",
    "\n",
    "        Args:\n",
    "            image_inputs (Union[str, Image.Image, List[Union[str, Image.Image]]]):\n",
    "                A single image (as a path, URL, or PIL.Image) or a list of images.\n",
    "            batch_size (int): The number of images to process in a single mini-batch.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray:\n",
    "                A NumPy array containing the embeddings.\n",
    "                - For a single image, the shape is (embedding_dim,).\n",
    "                - For a list of images, the shape is (num_images, embedding_dim).\n",
    "        \"\"\"\n",
    "        if not isinstance(image_inputs, list):\n",
    "            image_inputs = [image_inputs]\n",
    "            is_single_image = True\n",
    "        else:\n",
    "            is_single_image = False\n",
    "            \n",
    "        all_embeddings = []\n",
    "\n",
    "        for i in range(0, len(image_inputs), batch_size):\n",
    "            batch_inputs = image_inputs[i:i + batch_size]\n",
    "\n",
    "            image_tensors = [self._load_and_preprocess_image(img) for img in batch_inputs]\n",
    "            batch_tensor = torch.stack(image_tensors).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model(batch_tensor)\n",
    "                \n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "        if not all_embeddings:\n",
    "            return np.array([])\n",
    "\n",
    "        final_embeddings = np.vstack(all_embeddings)\n",
    "        \n",
    "        if is_single_image:\n",
    "            return final_embeddings.squeeze(0)\n",
    "        else:\n",
    "            return final_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b5c64",
   "metadata": {
    "papermill": {
     "duration": 0.010857,
     "end_time": "2025-06-27T21:43:34.430102",
     "exception": false,
     "start_time": "2025-06-27T21:43:34.419245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_cloth_embeddings(text_file_path: str, clothes: List[str], embeddings: List[str]):\n",
    "    with open(text_file_path, \"a\", encoding='utf-8') as file:\n",
    "        for idx in range(len(clothes)):\n",
    "            cloth_str_embeddings = ' '.join(map(str, embeddings[idx].tolist()))\n",
    "            file.write(f\"{clothes[idx]} {cloth_str_embeddings}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e3bb7",
   "metadata": {
    "papermill": {
     "duration": 0.005159,
     "end_time": "2025-06-27T21:43:34.440562",
     "exception": false,
     "start_time": "2025-06-27T21:43:34.435403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "**Encode clothes with large DinoV2 model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a228b",
   "metadata": {
    "papermill": {
     "duration": 11.459627,
     "end_time": "2025-06-27T21:43:45.905401",
     "exception": false,
     "start_time": "2025-06-27T21:43:34.445774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedder_large = DinoV2ImageEncoder(\"dinov2_vitl14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccb376",
   "metadata": {
    "papermill": {
     "duration": 2086.791192,
     "end_time": "2025-06-27T22:18:32.704273",
     "exception": false,
     "start_time": "2025-06-27T21:43:45.913081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch = 640\n",
    "output_file = \"/kaggle/working/dino_large_embeddings.txt\"\n",
    "# clear file\n",
    "with open(output_file, \"w\", encoding='utf-8') as file:\n",
    "    file.write('')\n",
    "# process clothes by batches\n",
    "for start_idx in range(0, len(clothes), global_batch):\n",
    "    clothes_batch = clothes[start_idx:start_idx+global_batch]\n",
    "    clothes_names_batch = extracted_files[start_idx:start_idx+global_batch]\n",
    "    embeddings_batch = embedder_large.encode(clothes_batch)\n",
    "    save_cloth_embeddings(output_file, clothes_names_batch, embeddings_batch)\n",
    "    print(f\"{start_idx+global_batch} clothes are encoded and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53494ee1",
   "metadata": {
    "papermill": {
     "duration": 0.010401,
     "end_time": "2025-06-27T22:18:32.725278",
     "exception": false,
     "start_time": "2025-06-27T22:18:32.714877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "**Encode clothes with base DinoV2 model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a2a8a",
   "metadata": {
    "papermill": {
     "duration": 3.256345,
     "end_time": "2025-06-27T22:18:35.992446",
     "exception": false,
     "start_time": "2025-06-27T22:18:32.736101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedder_base = DinoV2ImageEncoder(\"dinov2_vitb14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61224781",
   "metadata": {
    "papermill": {
     "duration": 941.504105,
     "end_time": "2025-06-27T22:34:17.508558",
     "exception": false,
     "start_time": "2025-06-27T22:18:36.004453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch = 1280\n",
    "output_file = \"/kaggle/working/dino_base_embeddings.txt\"\n",
    "# clear file\n",
    "with open(output_file, \"w\", encoding='utf-8') as file:\n",
    "    file.write('')\n",
    "# process clothes by batches\n",
    "for start_idx in range(0, len(clothes), global_batch):\n",
    "    clothes_batch = clothes[start_idx:start_idx+global_batch]\n",
    "    clothes_names_batch = extracted_files[start_idx:start_idx+global_batch]\n",
    "    embeddings_batch = embedder_base.encode(clothes_batch)\n",
    "    save_cloth_embeddings(output_file, clothes_names_batch, embeddings_batch)\n",
    "    print(f\"{start_idx+global_batch} clothes are encoded and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a91d4",
   "metadata": {
    "papermill": {
     "duration": 0.012793,
     "end_time": "2025-06-27T22:34:17.535272",
     "exception": false,
     "start_time": "2025-06-27T22:34:17.522479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "**Encode clothes with small DinoV2 model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ac9bf",
   "metadata": {
    "papermill": {
     "duration": 1.204461,
     "end_time": "2025-06-27T22:34:18.752658",
     "exception": false,
     "start_time": "2025-06-27T22:34:17.548197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedder_small = DinoV2ImageEncoder(\"dinov2_vits14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f79fad",
   "metadata": {
    "papermill": {
     "duration": 610.757091,
     "end_time": "2025-06-27T22:44:29.523914",
     "exception": false,
     "start_time": "2025-06-27T22:34:18.766823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch = 1280\n",
    "output_file = \"/kaggle/working/dino_small_embeddings.txt\"\n",
    "# clear file\n",
    "with open(output_file, \"w\", encoding='utf-8') as file:\n",
    "    file.write('')\n",
    "# process clothes by batches\n",
    "for start_idx in range(0, len(clothes), global_batch):\n",
    "    clothes_batch = clothes[start_idx:start_idx+global_batch]\n",
    "    clothes_names_batch = extracted_files[start_idx:start_idx+global_batch]\n",
    "    embeddings_batch = embedder_small.encode(clothes_batch)\n",
    "    save_cloth_embeddings(output_file, clothes_names_batch, embeddings_batch)\n",
    "    print(f\"{start_idx+global_batch} clothes are encoded and saved!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 247684794,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3696.618297,
   "end_time": "2025-06-27T22:44:32.028140",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-27T21:42:55.409843",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
